# Alloy - Telemetry collector and pipeline orchestrator
# Replaces Prometheus for metric collection
# Collects logs, metrics, and traces from all sources
# Routes to Mimir (metrics), Loki (logs), and Tempo (traces)

# Alloy configuration - must be under alloy.configMap.content for Helm chart
alloy:
  configMap:
    # Custom Alloy River configuration
    content: |
      logging {
        level  = "info"
        format = "json"
      }

      // Prometheus scrape configs for metrics collection
      prometheus.scrape "kubernetes_pods" {
        targets = discovery.kubernetes.pods.targets
        scrape_interval = "15s"
        job_name = "kubernetes-pods"
        forward_to = [prometheus.relabel.kubernetes_pods.receiver]
      }

      prometheus.scrape "kubernetes_nodes" {
        targets = discovery.kubernetes.nodes.targets
        scrape_interval = "15s"
        job_name = "kubernetes-nodes"
        forward_to = [prometheus.relabel.kubernetes_nodes.receiver]
      }

      // Kubernetes service discovery
      discovery.kubernetes "pods" {
        role = "pod"
      }

      discovery.kubernetes "nodes" {
        role = "node"
      }

      // Relabeling for Kubernetes metadata
      prometheus.relabel "kubernetes_pods" {
        forward_to = [prometheus.remote_write.mimir.receiver]
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label = "container"
        }
      }

      prometheus.relabel "kubernetes_nodes" {
        forward_to = [prometheus.remote_write.mimir.receiver]
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          target_label = "node"
        }
      }

      // Send metrics to Mimir
      prometheus.remote_write "mimir" {
        endpoint {
          url = "http://mimir-distributor.observability.svc.cluster.local:8080/api/v1/push"
        }
      }

      // Kubernetes log collection
      loki.source.kubernetes "pods" {
        targets = discovery.kubernetes.pods.targets
        forward_to = [loki.relabel.kubernetes_logs.receiver]
      }

      // Log relabeling for metadata
      loki.relabel "kubernetes_logs" {
        forward_to = [loki.write.loki.receiver]
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label = "container"
        }
      }

      // Send logs to Loki
      loki.write "loki" {
        endpoint {
          url = "http://loki-gateway.observability.svc.cluster.local/loki/api/v1/push"
        }
      }

      // OTLP gRPC receiver for application traces and metrics
      otelcol.receiver.otlp "default" {
        grpc {
          endpoint = "0.0.0.0:4317"
        }
        http {
          endpoint = "0.0.0.0:4318"
        }
        output {
          traces  = [otelcol.processor.batch.default.input]
          metrics = [otelcol.processor.batch.default.input]
        }
      }

      // Batch processor for better performance
      otelcol.processor.batch "default" {
        output {
          traces  = [otelcol.exporter.otlp.tempo.input]
          metrics = [otelcol.exporter.prometheus.mimir.input]
        }
      }

      // Export traces to Tempo distributor (OTLP endpoint)
      otelcol.exporter.otlp "tempo" {
        client {
          endpoint = "tempo-distributor.observability.svc.cluster.local:4317"
          tls {
            insecure = true
          }
        }
      }

      // Export OTLP metrics to Prometheus remote write format for Mimir
      otelcol.exporter.prometheus "mimir" {
        forward_to = [prometheus.remote_write.mimir.receiver]
      }

  # Extra ports for OTLP receivers (container ports)
  extraPorts:
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP

# Cluster role for pod discovery and scraping
serviceAccount:
  create: true
  name: alloy

# ClusterRole for Kubernetes discovery
rbac:
  create: true

# Service configuration to expose OTLP ports
controller:
  type: deployment
  replicas: 1

# Alloy DaemonSet configuration
daemonset:
  enabled: false

# Alloy StatefulSet configuration (optional, for centralized collection)
statefulset:
  enabled: false
