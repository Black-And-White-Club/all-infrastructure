# Alloy - Telemetry collector and pipeline orchestrator
# Replaces Prometheus for metric collection
# Collects logs, metrics, and traces from all sources
# Routes to Mimir (metrics), Loki (logs), and Tempo (traces)

# Alloy configuration - must be under alloy.configMap.content for Helm chart
alloy:
  configMap:
    # Custom Alloy River configuration
    content: |
      logging {
        level  = "info"
        format = "json"
      }

      // Prometheus scrape configs for metrics collection
      prometheus.scrape "kubernetes_pods" {
        targets = discovery.kubernetes.pods.targets
        scrape_interval = "15s"
        job_name = "kubernetes-pods"
        forward_to = [prometheus.relabel.kubernetes_pods.receiver]
      }

      prometheus.scrape "kubernetes_nodes" {
        targets = discovery.kubernetes.nodes.targets
        scrape_interval = "15s"
        job_name = "kubernetes-nodes"
        forward_to = [prometheus.relabel.kubernetes_nodes.receiver]
      }

      // Kubernetes service discovery
      discovery.kubernetes "pods" {
        role = "pod"
      }

      discovery.kubernetes "nodes" {
        role = "node"
      }

      // Relabeling for Kubernetes metadata
      prometheus.relabel "kubernetes_pods" {
        forward_to = [prometheus.remote_write.mimir.receiver]
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label = "container"
        }
      }

      prometheus.relabel "kubernetes_nodes" {
        forward_to = [prometheus.remote_write.mimir.receiver]
        rule {
          source_labels = ["__meta_kubernetes_node_name"]
          target_label = "node"
        }
      }

      // Send metrics to Mimir
      prometheus.remote_write "mimir" {
        endpoint {
          url = "http://mimir-distributor.observability.svc.cluster.local:8080/api/v1/push"
        }
      }

      // Discover pods for log collection
      discovery.kubernetes "pod_logs" {
        role = "pod"
      }

      // Relabel discovery targets for log file paths
      discovery.relabel "pod_logs" {
        targets = discovery.kubernetes.pod_logs.targets

        // Only collect from pods on this node
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          regex         = env("HOSTNAME")
          action        = "keep"
        }

        // Set log file path: /var/log/pods/{namespace}_{pod}_{uid}/{container}/*.log
        rule {
          source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_name", "__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
          target_label  = "__path__"
          separator     = "/"
          replacement   = "/var/log/pods/$1_$2_$3/$4/*.log"
        }

        // Keep pod name
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label  = "pod"
        }

        // Keep namespace
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label  = "namespace"
        }

        // Keep container name
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label  = "container"
        }

        // Keep node name
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          target_label  = "node"
        }

        // Keep app label
        rule {
          source_labels = ["__meta_kubernetes_pod_label_app"]
          target_label  = "app"
        }

        // Keep app.kubernetes.io/name label
        rule {
          source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          target_label  = "app_name"
        }
      }

      // Collect logs from files
      loki.source.file "pod_logs" {
        targets    = discovery.relabel.pod_logs.output
        forward_to = [loki.process.pod_logs.receiver]
      }

      // Process logs (parse JSON if applicable)
      loki.process "pod_logs" {
        forward_to = [loki.write.loki.receiver]

        // Parse container log format (CRI format)
        stage.cri {}

        // Try to parse JSON logs
        stage.json {
          expressions = {
            level   = "level",
            msg     = "msg",
            time    = "time",
            trace_id = "trace_id",
          }
        }

        // Add level label if parsed
        stage.labels {
          values = {
            level = "",
          }
        }
      }

      // Send logs to Loki
      loki.write "loki" {
        endpoint {
          url = "http://loki-gateway.observability.svc.cluster.local/loki/api/v1/push"
        }
      }

      // OTLP gRPC receiver for application traces and metrics
      otelcol.receiver.otlp "default" {
        grpc {
          endpoint = "0.0.0.0:4317"
        }
        http {
          endpoint = "0.0.0.0:4318"
        }
        output {
          traces  = [otelcol.processor.batch.default.input]
          metrics = [otelcol.processor.batch.default.input]
        }
      }

      // Batch processor for better performance
      otelcol.processor.batch "default" {
        output {
          traces  = [otelcol.exporter.otlp.tempo.input]
          metrics = [otelcol.exporter.prometheus.mimir.input]
        }
      }

      // Export traces to Tempo distributor (OTLP endpoint)
      otelcol.exporter.otlp "tempo" {
        client {
          endpoint = "tempo-distributor.observability.svc.cluster.local:4317"
          tls {
            insecure = true
          }
        }
      }

      // Export OTLP metrics to Prometheus remote write format for Mimir
      otelcol.exporter.prometheus "mimir" {
        forward_to = [prometheus.remote_write.mimir.receiver]
      }

  # Mount host log directories for log collection
  mounts:
    varlog: true
    dockercontainers: true

  # Extra ports for OTLP receivers (container ports)
  extraPorts:
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP

# Cluster role for pod discovery and scraping
serviceAccount:
  create: true
  name: alloy

# ClusterRole for Kubernetes discovery
rbac:
  create: true

# Service configuration to expose OTLP ports
controller:
  type: daemonset
  resources:
    requests:
      cpu: 25m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi

# Alloy StatefulSet configuration (optional, for centralized collection)
statefulset:
  enabled: false
