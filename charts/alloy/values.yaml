# Alloy - OTEL-first telemetry collector
# Apps emit logs/metrics/traces via OTLP only
# Infrastructure metrics scraped via Prometheus (kube-state-metrics, node-exporter)

alloy:
  configMap:
    content: |
      logging {
        level  = "info"
        format = "json"
      }

      /////////////////////////////////////////
      // OTLP RECEIVER - ALL APPLICATION SIGNALS
      /////////////////////////////////////////

      otelcol.receiver.otlp "default" {
        grpc {
          endpoint = "0.0.0.0:4317"
        }
        http {
          endpoint = "0.0.0.0:4318"
        }
        output {
          // Pointing to the new transform processor
          logs    = [otelcol.processor.transform.default.input]
          metrics = [otelcol.processor.transform.default.input]
          traces  = [otelcol.processor.transform.default.input]
        }
      }

      /////////////////////////////////////////
      // TRANSFORM PROCESSOR - ENRICH ALL SIGNALS
      // (Replaces the invalid otelcol.processor.resource)
      /////////////////////////////////////////

      otelcol.processor.transform "default" {
        error_mode = "ignore"

        // 1. Logs: Set cluster name in resource attributes
        log_statements {
          context = "resource"
          statements = [
            "set(attributes[\"k8s.cluster.name\"], \"production\")",
          ]
        }

        // 2. Metrics: Set cluster name in resource attributes
        metric_statements {
          context = "resource"
          statements = [
            "set(attributes[\"k8s.cluster.name\"], \"production\")",
          ]
        }

        // 3. Traces: Set cluster name in resource attributes
        trace_statements {
          context = "resource"
          statements = [
            "set(attributes[\"k8s.cluster.name\"], \"production\")",
          ]
        }

        output {
          logs    = [otelcol.processor.attributes.logs.input]
          metrics = [otelcol.processor.batch.default.input]
          traces  = [otelcol.processor.batch.default.input]
        }
      }

      /////////////////////////////////////////
      // ATTRIBUTES PROCESSOR - LOG LABEL HINTS
      /////////////////////////////////////////

      otelcol.processor.attributes "logs" {
        // Hint attributes for Loki label extraction (low-cardinality only)
        action {
          key    = "loki.attribute.labels"
          action = "insert"
          value  = "service.name,deployment.environment,k8s.namespace.name"
        }
        output {
          logs = [otelcol.processor.batch.logs.input]
        }
      }

      /////////////////////////////////////////
      // BATCH PROCESSORS
      /////////////////////////////////////////

      otelcol.processor.batch "default" {
        timeout         = "5s"
        send_batch_size = 0
        output {
          traces  = [otelcol.exporter.otlp.tempo.input]
          metrics = [otelcol.exporter.prometheus.mimir.input]
        }
      }

      otelcol.processor.batch "logs" {
        timeout         = "5s"
        send_batch_size = 0
        output {
          logs = [otelcol.exporter.loki.default.input]
        }
      }

      /////////////////////////////////////////
      // EXPORTERS
      /////////////////////////////////////////

      otelcol.exporter.otlp "tempo" {
        client {
          endpoint = "tempo-distributor.observability.svc.cluster.local:4317"
          tls {
            insecure = true
          }
        }
      }

      otelcol.exporter.loki "default" {
        forward_to = [loki.write.default.receiver]
      }

      otelcol.exporter.prometheus "mimir" {
        forward_to = [prometheus.remote_write.mimir.receiver]
      }

      /////////////////////////////////////////
      // BACKEND WRITERS
      /////////////////////////////////////////

      loki.write "default" {
        endpoint {
          url = "http://loki-gateway.observability.svc.cluster.local/loki/api/v1/push"
        }
      }

      prometheus.remote_write "mimir" {
        endpoint {
          url = "http://mimir-gateway.observability.svc.cluster.local/api/v1/push"
        }
      }

      /////////////////////////////////////////
      // INFRASTRUCTURE METRICS (NON-OTLP)
      /////////////////////////////////////////

      discovery.kubernetes "services" {
        role = "service"
      }

      discovery.kubernetes "endpoints" {
        role = "endpoints"
      }

      // kube-state-metrics
      prometheus.scrape "kube_state_metrics" {
        targets         = discovery.relabel.kube_state_metrics.output
        scrape_interval = "30s"
        job_name        = "kube-state-metrics"
        forward_to      = [prometheus.remote_write.mimir.receiver]
      }

      discovery.relabel "kube_state_metrics" {
        targets = discovery.kubernetes.services.targets
        rule {
          source_labels = ["__meta_kubernetes_service_name"]
          regex         = "kube-state-metrics"
          action        = "keep"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label  = "namespace"
        }
      }

      // node-exporter
      prometheus.scrape "node_exporter" {
        targets         = discovery.relabel.node_exporter.output
        scrape_interval = "30s"
        job_name        = "node-exporter"
        forward_to      = [prometheus.remote_write.mimir.receiver]
      }

      discovery.relabel "node_exporter" {
        targets = discovery.kubernetes.endpoints.targets
        rule {
          source_labels = ["__meta_kubernetes_service_name"]
          regex         = "node-exporter.*"
          action        = "keep"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          target_label  = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_node_name"]
          target_label  = "node"
        }
      }

  extraPorts:
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
      protocol: TCP
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP
    - name: internal-http
      port: 12347
      targetPort: 12345
      protocol: TCP

serviceAccount:
  create: true
  name: alloy

rbac:
  create: true

ingress:
  enabled: true
  className: nginx
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://frolf-bot.duckdns.org"
    nginx.ingress.kubernetes.io/cors-allow-methods: "POST, OPTIONS"
    nginx.ingress.kubernetes.io/cors-allow-headers: "Content-Type"
  hosts:
    - frolf-bot.duckdns.org
  extraPaths:
    - path: /v1
      pathType: Prefix
      backend:
        service:
          name: alloy
          port:
            number: 4318
  tls:
    - secretName: frolf-bot-pwa-tls
      hosts:
        - frolf-bot.duckdns.org

controller:
  type: daemonset
  resources:
    requests:
      cpu: 10m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi
